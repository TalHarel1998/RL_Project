{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "odNaDE1zyrL2",
        "APXSx7hg19TH",
        "Cu31YW_vslRB",
        "tdKcPR6qstZR",
        "zg7n0mTQb1Xj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalHarel1998/RL_Project/blob/main/Copy_of_OpenAi_Gym_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "#add \" > /dev/null 2>&1\" at the end of each command to ignore output\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE"
      },
      "source": [
        "Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "!apt-get update\n",
        "!pip install --upgrade setuptools\n",
        "!pip install \"gym[atari]\"==0.9.5\n",
        "!pip install opencv-python\n",
        "!pip install torch\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_FO_RkC3aBA"
      },
      "source": [
        "# ERRORS CAN BE IGNORED! all the dependency conflicts are \"won\" by gym!\n",
        "# (maybe because of the --force parameter ?)\n",
        "\n",
        "!pip install --force \"gym[atari]\"==0.9.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmls-nMlGgDE"
      },
      "source": [
        "# download, extract & import all available ROMs to gym \n",
        "!curl http://www.atarimania.com/roms/Roms.rar -o Roms.rar\n",
        "!unrar x Roms.rar\n",
        "!python -m atari_py.import_roms ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf RL_Project\n",
        "!git clone https://github.com/TalHarel1998/RL_Project.git\n",
        "%cd /content/RL_Project/dqn"
      ],
      "metadata": {
        "id": "vq3j5xh2ZNvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "print(gym.__version__)  # make sure it says 0.9.5\n",
        "from gym import logger as gymlogger\n",
        "from gym import wrappers\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes From Skeleton"
      ],
      "metadata": {
        "id": "gcQyErGTwDp4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M"
      },
      "source": [
        "# Pong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "source": [
        "# # Get Atari games.\n",
        "# benchmark = gym.benchmark_spec('Atari40M')\n",
        "# task = benchmark.tasks[3]\n",
        "# env = gym.make(task.env_id)\n",
        "\n",
        "env = gym.make('PongNoFrameskip-v4')  # this command should succeeds\n",
        "\n",
        "from utils.atari_wrapper import ProcessFrame84\n",
        "env = ProcessFrame84(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulating random exploation"
      ],
      "metadata": {
        "id": "Cu31YW_vslRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "replay_buffer = ReplayBuffer(100, 7)\n",
        "\n",
        "last_obs = env.reset()\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  stored_idx = replay_buffer.store_frame(last_obs)\n",
        "\n",
        "  # action = random.randrange(env.action_space.n)\n",
        "\n",
        "  action = \n",
        "\n",
        "  obs, reward, done, info = env.step(action)\n",
        "\n",
        "  print(obs.shape)\n",
        "\n",
        "  encoded_obs = replay_buffer.encode_recent_observation()\n",
        "\n",
        "  replay_buffer.store_effect(stored_idx, action, reward, done)\n",
        "\n",
        "  last_obs = obs"
      ],
      "metadata": {
        "id": "FsDCbHw-vgge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## attempting to run the model"
      ],
      "metadata": {
        "id": "tdKcPR6qstZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from dqn_model import DQN\n",
        "\n",
        "obs_batch = torch.from_numpy(np.random.rand(10, 1, 84, 84)).type(torch.float32)\n",
        "print(obs_batch.shape)\n",
        "\n",
        "model = DQN(in_channels = 1)\n",
        "res = model(obs_batch)\n",
        "\n",
        "print(res.data.max(1)[0])\n",
        "print([max(res.data[i]) for i in range(0,10,2)])\n"
      ],
      "metadata": {
        "id": "dAnEYecSeyCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.autograd as autograd\n",
        "from dqn_model import DQN\n",
        "\n",
        "class Variable(autograd.Variable):\n",
        "    def __init__(self, data, *args, **kwargs):\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "        super(Variable, self).__init__(data, *args, **kwargs)\n",
        "\n",
        "def select_epilson_greedy_action(model, obs):\n",
        "        obs = torch.from_numpy(obs).type(torch.float32).unsqueeze(0) / 255.0\n",
        "        # with torch.no_grad() variable is only used in inference mode, i.e. don’t save the history\n",
        "        with torch.no_grad():\n",
        "            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n",
        "        \n",
        "model = DQN(in_channels = 1)\n",
        "\n",
        "last_obs = env.reset()\n",
        "\n",
        "obs, reward, done, info = env.step(0)\n",
        "\n",
        "select_epilson_greedy_action(model, np.moveaxis(obs, -1, 0))"
      ],
      "metadata": {
        "id": "fo-yftmhj9uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "obs_batch = torch.from_numpy(np.random.rand(10, 1, 84, 84)).type(torch.float32)\n",
        "print(obs_batch.shape)\n",
        "\n",
        "model = DQN(in_channels = 1)\n",
        "\n",
        "loss = torch.nn.MSELoss\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "oQbpMxrOl4Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = sum(model(obs_batch).max(1)[0])\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(res.backward())"
      ],
      "metadata": {
        "id": "634ooAsimLZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## running main"
      ],
      "metadata": {
        "id": "ss2igk4mGc7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "id": "F9ixh0MuGe9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code that runs before t==1 (main.py + imports + initializations)\n"
      ],
      "metadata": {
        "id": "zg7n0mTQb1Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ content of main.py ------------------------\n",
        "\n",
        "import gym\n",
        "import torch.optim as optim\n",
        "\n",
        "from dqn_model import DQN\n",
        "from dqn_learn import OptimizerSpec, dqn_learing\n",
        "from utils.gym import get_env, get_wrapper_by_name\n",
        "from utils.schedule import LinearSchedule\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "REPLAY_BUFFER_SIZE = 1000000\n",
        "LEARNING_STARTS = 50000\n",
        "LEARNING_FREQ = 4\n",
        "FRAME_HISTORY_LEN = 1           #TODO: change back to 4 !\n",
        "TARGER_UPDATE_FREQ = 10000\n",
        "LEARNING_RATE = 0.00025\n",
        "ALPHA = 0.95\n",
        "EPS = 0.01\n",
        "\n",
        "\n",
        "\n",
        "# Get Atari games.\n",
        "benchmark = gym.benchmark_spec('Atari40M')\n",
        "\n",
        "# Change the index to select a different game.\n",
        "task = benchmark.tasks[3]\n",
        "\n",
        "# Run training\n",
        "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
        "env = get_env(task, seed)\n",
        "\n",
        "num_timesteps = task.max_timesteps\n",
        "\n",
        "# ---------------------------------- content of main ----------------------------------\n",
        "\n",
        "def stopping_criterion(env):\n",
        "    # notice that here t is the number of steps of the wrapped env,\n",
        "    # which is different from the number of steps in the underlying env\n",
        "    return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
        "\n",
        "optimizer_spec = OptimizerSpec(\n",
        "    constructor=optim.RMSprop,\n",
        "    kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
        ")\n",
        "\n",
        "exploration_schedule = LinearSchedule(1000000, 0.1)\n",
        "\n",
        "q_func=DQN\n",
        "optimizer_spec=optimizer_spec\n",
        "exploration=exploration_schedule\n",
        "stopping_criterion=stopping_criterion\n",
        "replay_buffer_size=REPLAY_BUFFER_SIZE\n",
        "batch_size=BATCH_SIZE\n",
        "gamma=GAMMA\n",
        "learning_starts=LEARNING_STARTS\n",
        "learning_freq=LEARNING_FREQ\n",
        "frame_history_len=FRAME_HISTORY_LEN\n",
        "target_update_freq=TARGER_UPDATE_FREQ\n",
        "\n",
        "# ---------------------------------- imports of dqn_learn ----------------------------------\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import random\n",
        "import gym.spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "\n",
        "from utils.replay_buffer import ReplayBuffer\n",
        "from utils.gym import get_wrapper_by_name\n",
        "\n",
        "# ---------------------------------- init content of dqn_learn ----------------------------------\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "if len(env.observation_space.shape) == 1:\n",
        "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
        "    input_arg = env.observation_space.shape[0]\n",
        "else:\n",
        "    img_h, img_w, img_c = env.observation_space.shape\n",
        "    input_arg = frame_history_len * img_c\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Construct an epilson greedy policy with given exploration schedule\n",
        "def select_epilson_greedy_action(model, obs, t):\n",
        "    sample = random.random()\n",
        "    eps_threshold = exploration.value(t)\n",
        "    if sample > eps_threshold:\n",
        "        obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n",
        "        # with torch.no_grad() variable is only used in inference mode, i.e. don’t save the history\n",
        "        with torch.no_grad():\n",
        "            return model(Variable(obs)).data.max(1)[1].cpu()\n",
        "    else:\n",
        "        return torch.IntTensor([[random.randrange(num_actions)]])\n"
      ],
      "metadata": {
        "id": "9GjKAoj0R3t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### exploration&training loop"
      ],
      "metadata": {
        "id": "rzsp0uYNcNfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize target q function and q function, i.e. build the model.\n",
        "######\n",
        "\n",
        "# YOUR CODE HERE\n",
        "Q        = q_func(input_arg, num_actions)\n",
        "target_Q = q_func(input_arg, num_actions)\n",
        "\n",
        "######\n",
        "\n",
        "# Construct Q network optimizer function\n",
        "optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
        "\n",
        "# Construct the replay buffer\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
        "\n",
        "\n",
        "###############\n",
        "# RUN ENV     #\n",
        "###############\n",
        "num_param_updates = 0\n",
        "mean_episode_reward = -float('nan')\n",
        "best_mean_episode_reward = -float('inf')\n",
        "last_obs = env.reset()\n",
        "LOG_EVERY_N_STEPS = 10000\n",
        "\n",
        "for t in count():\n",
        "    ### 1. Check stopping criterion\n",
        "    if stopping_criterion is not None and stopping_criterion(env):\n",
        "        break\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Steps the environment forward one step\n",
        "    action = select_epilson_greedy_action(Q, np.moveaxis(last_obs, -1, 0), t)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # Stors and updates replay_buffer's variables due the latest observation\n",
        "    stored_idx  = replay_buffer.store_frame(last_obs)\n",
        "    # TODO: do we need the encoded_obs? maybe we'll have to use it during the learning process / \n",
        "    if t > 0:\n",
        "        encoded_obs = replay_buffer.encode_recent_observation()\n",
        "    replay_buffer.store_effect(stored_idx, action, reward, done)\n",
        "\n",
        "    if done:\n",
        "        last_obs = env.reset()\n",
        "    else:\n",
        "        last_obs = obs\n",
        "\n",
        "    #####\n",
        "\n",
        "\n",
        "    if (t > learning_starts and\n",
        "            t % learning_freq == 0 and\n",
        "            replay_buffer.can_sample(batch_size)):\n",
        "        \n",
        "        print(f\"breaking at t={t}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "FRlGj86QSEx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
        "        \n",
        "obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = \\\n",
        "    torch.from_numpy(obs_batch).type(dtype) / 225.0, \\\n",
        "    torch.from_numpy(act_batch).type(dtype), \\\n",
        "    torch.from_numpy(rew_batch).type(dtype), \\\n",
        "    torch.from_numpy(next_obs_batch).type(dtype) / 255.0, \\\n",
        "    torch.from_numpy(done_mask)\n",
        "\n",
        "obs_batch, next_obs_batch = Variable(obs_batch), Variable(next_obs_batch)\n",
        "\n",
        "# compute bellman error\n",
        "Q_next = Q(next_obs_batch).data.max(1)[0]\n",
        "Q_next = Q_next * done_mask # if obs.done=True, Q_next is obviously zero\n",
        "Q_forward = Q(obs_batch) \n",
        "\n",
        "expanded_act_batch = act_batch.unsqueeze(1).expand(-1, Q_forward.size(1)).type(torch.int64)\n",
        "sliced_Q_forward = torch.gather(Q_forward, 1, expanded_act_batch)[:,0]\n",
        "\n",
        "sliced_Q_forward.shape\n",
        "\n",
        "bellman_error = rew_batch + gamma * Q_next - torch.detach(sliced_Q_forward).numpy()\n",
        "bellman_error = np.clip(bellman_error, -1, 1) * -1\n",
        "bellman_error = torch.sum(bellman_error)\n",
        "\n",
        "# # make an optimization step\n",
        "# optimizer.zero_grad()\n",
        "# bellman_error.backward()\n",
        "# optimizer.step()\n",
        "\n",
        "# if (num_param_updates % target_update_freq) == 0:\n",
        "#     target_Q.load_state_dict(Q.state_dict())\n",
        "\n",
        "# num_param_updates += 1"
      ],
      "metadata": {
        "id": "5P1EUiVcUZS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "bellman_error.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "gdScQyG_ZWE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}